# -*- coding: utf-8 -*-

def iris_nn():
    """Untitled1.ipynb

    Automatically generated by Colaboratory.

    Original file is located at
        https://colab.research.google.com/drive/1VP7qclm8kv1T7PDDy7l2cW1IjjnlUBCr
    """
    import numpy as np
    epochnum = 1000
    from sklearn.datasets import load_iris
    iris = load_iris()

    # print(iris)

    # print(len(iris.data))
    # print(len(iris.target))

    from chainer import functions as F
    from chainer import links as L
    import chainer

    from chainer.datasets import split_dataset_random
    
    class MyNN(chainer.Chain):

      def __init__(self, output_label):
        super(MyNN, self).__init__()
        weight_initializer = chainer.initializers.Normal(scale=0.08, dtype=None)

        with self.init_scope():
              self.l1 = L.Linear(4, 64)
              self.l2 = L.Linear(None, 64)
              self.l3 = L.Linear(None, 64)
              self.l4 = L.Linear(None, 64)
              self.l5 = L.Linear(None, 64)
              self.fc1 = L.Linear(None, 128)
              self.fc2 = L.Linear(None, output_label)

      def __call__(self, x, t=None, train=True):
        h1 = self.l1(x)
        h2 = self.l2(h1)
        h3 = F.sigmoid(h2)
        h6 = self.fc1(h3)
        h7 = self.fc2(h6)

        return h7 # y if train else F.softmax(h5)


    model = MyNN(3)
    model = L.Classifier(model)
    optimizer = chainer.optimizers.SGD()
    optimizer.setup(model)

    iris["data"] = iris["data"].astype(np.float32)

    data_and_labels = list(zip(iris["data"], iris["target"]))

    np.random.shuffle(data_and_labels)

    threshold = 0.7
    l = len(data_and_labels)
    lth = int(l*threshold)
    print(lth)
    print("************")
    data = [d[0] for d in data_and_labels]
    labels = [d[1] for d in data_and_labels]
    

    """ 
    classnum = np.max(labels)
    tmplabels = np.zeros((len(labels), classnum + 1), dtype=np.int)
    
    for i, j in zip(tmplabels, labels):
        i[j] = 1
    print(tmplabels)
    
    labels = tmplabels
    """

    train = chainer.datasets.tuple_dataset.TupleDataset(data[:lth], labels[:lth])
    test  = chainer.datasets.tuple_dataset.TupleDataset(data[lth:], labels[lth:])

    train, test = split_dataset_random(chainer.datasets.tuple_dataset.TupleDataset(data, labels))

    batch_size = 16
    test_batch_size = 32

    train_iter = chainer.iterators.SerialIterator(train, batch_size, shuffle=True)
    test_iter = chainer.iterators.SerialIterator(test, test_batch_size, repeat=False, shuffle=False)


    from chainer import training, datasets, iterators, optimizers
    from chainer.training import extensions
    from chainer.training.extensions import LogReport
    from chainer.backends import cuda

    updater = training.StandardUpdater(train_iter, optimizer)

    trainer = training.Trainer(updater, (epochnum, 'epoch'), out='result')
    trainer.extend(extensions.Evaluator(test_iter, model), name='validation')
    trainer.extend(extensions.LogReport())
    trainer.extend(extensions.PrintReport(
        ['epoch', 'main/loss', 'validation/main/loss',
         'main/accuracy', 'validation/main/accuracy', 'elapsed_time']))

    # trainer.extend(extensions.PrintReport(['epoch', 'val/main/loss']))
    trainer.extend(extensions.ProgressBar())


    trainer.run()

def xor_nn():
    import numpy as np
    epochnum = 6500
    from sklearn.datasets import load_iris


    from chainer import functions as F
    from chainer import links as L
    import chainer

    class MyNN(chainer.Chain):
        def __init__(self, output_label):
            super(MyNN, self).__init__()

            with self.init_scope():
                self.l1 = L.Linear(2, 56)
                self.fc1 = L.Linear(None, 24)
                self.fc2 = L.Linear(None, output_label)

        def __call__(self, x, t=None, train=True):
            h1 = self.l1(x)
            h1 = F.relu(h1)
            h2 = F.relu(self.fc1(h1))
            h3 = self.fc2(h2)

            return h3

    model = MyNN(2)
    model = L.Classifier(model)
    optimizer = chainer.optimizers.SGD()
    optimizer.setup(model)

    xordata = np.array([0.,0.,0.,1., 1,0, 1,1]).reshape(4, 2)
    print(xordata)
    xordata = xordata.astype(np.float32)
    xorans = np.array([0, 1, 1, 0], dtype=np.int32)

    train = chainer.datasets.tuple_dataset.TupleDataset(xordata, xorans)
    test = chainer.datasets.tuple_dataset.TupleDataset(xordata, xorans)

    batch_size = 4
    test_batch_size = 4

    train_iter = chainer.iterators.SerialIterator(train, batch_size, shuffle=True)
    test_iter = chainer.iterators.SerialIterator(test, test_batch_size, repeat=False, shuffle=False)

    from chainer import training, datasets, iterators, optimizers
    from chainer.training import extensions
    from chainer.training.extensions import LogReport
    from chainer.backends import cuda

    updater = training.StandardUpdater(train_iter, optimizer)  # loss_func=F.softmax_cross_entropy) これを指定するとうまく学習されない?

    trainer = training.Trainer(updater, (epochnum, 'epoch'), out='result')
    trainer.extend(extensions.Evaluator(test_iter, model), name='validation')
    trainer.extend(extensions.LogReport())
    trainer.extend(extensions.PrintReport(
        ['epoch', 'main/loss', 'validation/main/loss',
         'main/accuracy', 'validation/main/accuracy', 'elapsed_time']))

    # trainer.extend(extensions.PrintReport(['epoch', 'val/main/loss']))
    trainer.extend(extensions.ProgressBar())

    trainer.run()

iris_nn()
